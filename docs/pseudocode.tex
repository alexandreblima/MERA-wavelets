\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Wave6G Optimization Workflow: Executable Pseudocode}
\author{Generated by Codex Assistant}

\begin{document}

\maketitle

\begin{abstract}
This document captures the two core routines that actually run inside \texttt{Wave6G.jl}: the variational optimization loop (\texttt{optimize\_wavelet\_sparsity}) and the projection used after each gradient step (\texttt{TensorUpdateSVD}).  The pseudocode mirrors the current workflow while adopting the notation used in the accompanying paper, where the learnable isometries are denoted by the tensor family $U$.  Auxiliary tensors that are not actively optimised in the released code are intentionally omitted so the presentation remains faithful to what is reported in the text.
\end{abstract}

\section{Notation and Context}
We optimise a Multi-scale Entanglement Renormalization Ansatz (MERA) tailored to network telemetry signals.  Each tensor leg carries a \emph{bond dimension}, denoted by $\chi$, which is the number of latent channels flowing across that leg.  Increasing $\chi$ allows the wavelet representation to capture richer correlations at the cost of additional parameters; in signal-processing language, $\chi$ controls how many subband channels are preserved at each decomposition level.  The auxiliary dimension $\chi_{\text{mid}}$ corresponds to the intermediate legs of MERA disentanglers and determines the capacity of the lifting steps that decorrelate neighbour samples before down-sampling.

A note on level numbering: level~$1$ is the \emph{first} decomposition stage immediately after the raw signal, while level~$L$ is the coarsest one (obtained after $L$ rounds of decimation).  Every application of \textsc{MERAAnalyze} at level~$\ell$ yields paired approximation (low-pass) and detail (high-pass) vectors on each child leg.  Before applying the isometries $U^{(\ell)}$, we interleave these approximation/detail channels from the two child legs into a single vector $\mathbf{f}$ (cf. Appendix~\ref{sec:chi-two-example}).  The rows of $U^{(\ell)}$ therefore encode the different linear combinations of approximation-versus-detail information that survive to the coarser scale.  Larger $\chi$ means more independent mixtures of approximations and details can be propagated upward, enriching the multiscale description.

Throughout the pipeline, every level of \textsc{MERAAnalyze} yields paired vectors: an approximation branch (low-pass) and a detail branch (high-pass).  Prior to applying the isometries $U^{(\ell)}$, we interleave these approximation/detail channels from the two child legs into a single vector $\mathbf{f}$ (cf. Appendix~\ref{sec:chi-two-example}).  The rows of $U^{(\ell)}$ therefore encode the different linear combinations of \emph{approximation-versus-detail} information that survive to the coarser scale.  Larger $\chi$ means more independent mixtures of approximations and details can be propagated upward, enriching the multiscale description.

Bond dimensions upper-bound the rank that emerges when we flatten a tensor along a given leg.  For instance, a matrix $M \in \mathbb{C}^{m \times n}$ cannot have rank larger than $\min(m,n)$; here the “bond dimension” of each leg is just $m$ or $n$, respectively.  Likewise, each isometry $U^{(\ell)}$ in MERA can be matricised by grouping its two child legs on one axis and its parent leg on the other.  If $\chi=2$, only two independent combinations of child subbands can flow upward, regardless of how wide the child legs are; choosing $\chi=8$ relaxes that cap to eight independent mixtures, increasing the expressiveness of the learned multi-scale filters.  As a concrete illustration, setting $\chi=2$ means that, no matter how many coefficients the child legs contain, the matrix form of $U^{(\ell)}$ has rank at most $2$, so it passes at most two independent modes to the coarser scale.  With $\chi=8$ the matrix can reach rank up to $8$, enabling eight distinct mixtures of the child subbands.  The same rationale holds for $\chi_{\text{mid}}$ inside lifting blocks: it limits how many independent “decorrelation” channels the disentanglers can sustain before down-sampling.

\section{End-to-End Optimization Loop}
The optimization routine minimises a sparsity--fidelity objective using Adam.  The tensor family $U$ (scale isometries in the paper's notation) is the only learnable component exposed in this description; other auxiliary tensors present in the implementation do not receive gradient updates and are therefore omitted to avoid confusion.  The routine underlies \texttt{scripts/run\_varmera\_style\_optimization.jl}.

\begin{algorithm}[t]
  \caption{Variational MERA--Wavelet Optimization (\texttt{optimize\_wavelet\_sparsity})}
  \label{alg:mera_opt}
  \begin{algorithmic}[1]
    \Require $x \in \mathbb{R}^N$ (signal window), hierarchy depth $L$, bond dimensions $(\chi, \chi_{\text{mid}})$, hyperparameters $\texttt{numiter}$, learning rate $\eta$, sparsity weight $\lambda_{\text{sparse}}$, MSE weight $\lambda_{\text{MSE}}$, optional warm start $U_0$.
    \State Initialise $U$ with the warm start if available; else call \textsc{InitRandomTensors}$(L, \chi, \chi_{\text{mid}})$ and keep the isometric branch.
    \State Cast $U$ to $\mathrm{Complex32}$ (i.e., $\mathrm{Complex}\{\mathrm{Float32}\}$, alias \texttt{ComplexF32} in Julia) and move to GPU if CUDA is active.
    \State Set up Adam state $(m, v)$ over the parameter tuple $(U)$ via \textsc{Flux.setup}.
    \For{$k = 1$ to $\texttt{numiter}$}
      \State $(a, \{d^{(\ell)}\}_{\ell=1}^L) \gets$ \textsc{MERAAnalyze}$(x, U)$ \Comment{$d^{(\ell)}$ are detail bands}
      \State $\mathcal{L}_{\text{sparse}} \gets \frac{1}{L} \sum_{\ell=1}^L \lVert d^{(\ell)} \rVert_1$ \Comment{mean $\ell_1$ sparsity}
      \If{$\lambda_{\text{MSE}} > 0$}
        \State $\hat{x} \gets$ \textsc{MERASynthesize}$(a, \{d^{(\ell)}\}, U)$
        \State $\mathcal{L}_{\text{MSE}} \gets \frac{1}{N} \lVert \hat{x} - x \rVert_2^2$
      \Else
        \State $\mathcal{L}_{\text{MSE}} \gets 0$
      \EndIf
      \State $\mathcal{L} \gets \lambda_{\text{sparse}} \mathcal{L}_{\text{sparse}} + \lambda_{\text{MSE}} \mathcal{L}_{\text{MSE}}$
      \State $\nabla_U \gets \nabla_{U} \mathcal{L}$ via Zygote reverse-mode AD
      \State $(U, m, v) \gets$ \textsc{AdamUpdate}$(U, \nabla_U, m, v, k)$
      \ForAll{$\ell \in \{1, \dots, L\}$} \Comment{restore isometry after Adam step}
        \State $U[\ell] \gets$ \textsc{TensorUpdateSVD}$(U[\ell], 1)$
      \EndFor
    \EndFor
    \State \Return $U$ (final tensors) and the loss trace $\{\mathcal{L}_k\}$
  \end{algorithmic}
\end{algorithm}

\paragraph{Operational notes.}%
\begin{itemize}
  \item The optimisation focuses exclusively on the isometric branch $U$, matching the simplified presentation adopted in the manuscript.
  \item Adopting $\mathrm{Complex32}$ ensures compatibility with the implementation, where the alias \texttt{ComplexF32} provides a common complex precision across CPU and GPU back-ends.
  \item \textsc{MERAAnalyze}/\textsc{MERASynthesize} are shown with the single argument $U$ to reinforce that only this tensor family is exposed in the paper.
  \item The inner loop mirrors the \texttt{Flux.withgradient} block in the Julia code, including the optional reconstruction error branch.
\end{itemize}

\section{Isometric Projection Routine}
Each gradient step may break the orthonormal constraints on the tensor legs.  The helper \textsc{TensorUpdateSVD} is a polar decomposition that retracts a tensor back onto the Stiefel manifold along a specified index.

\begin{algorithm}[t]
  \caption{TensorUpdateSVD: Isometric Projection (\texttt{TensorUpdateSVD})}
  \label{alg:tensor_update_svd}
  \begin{algorithmic}[1]
    \Require Tensor $T \in \mathbb{C}^{n_1 \times \cdots \times n_D}$, axis $\alpha \in \{1, \dots, D\}$
    \State $\pi \gets$ permutation that moves axis $\alpha$ to the last position
    \State $T_\pi \gets \textsc{PermuteDims}(T, \pi)$
    \State $(n_{\text{row}}, n_{\text{col}}) \gets \left(\prod_{i=1}^{D-1} n_i^\pi,\; n_D^\pi\right)$
    \State $A \gets \textsc{Reshape}(T_\pi, n_{\text{row}}, n_{\text{col}})$
    \State $G \gets A^\dagger A$ \Comment{column Gram matrix}
    \State $(Q, \Lambda) \gets \textsc{EigenDecompose}(G)$ with $\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_{n_{\text{col}}})$
    \State $\tau \gets \max(\Lambda) \cdot \varepsilon$ \Comment{machine-precision threshold}
    \State $\tilde{\Lambda}_{ii} \gets \begin{cases} \lambda_i^{-1/2}, & \lambda_i > \tau \\ 0, & \text{otherwise} \end{cases}$
    \State $U \gets A Q \tilde{\Lambda}$ \Comment{polar factor $A (A^\dagger A)^{-1/2}$}
    \State $U_\pi \gets \textsc{Reshape}(U, n_1^\pi, \dots, n_D^\pi)$
    \State $T_{\text{iso}} \gets \textsc{PermuteDims}(U_\pi, \pi^{-1})$
    \State \Return $T_{\text{iso}}$ \Comment{columns along axis $\alpha$ satisfy $T_{\text{iso}}^\dagger T_{\text{iso}} = I$}
  \end{algorithmic}
\end{algorithm}

\paragraph{Interpretation.}%
\begin{itemize}
  \item The reshaping step gathers every leg except $\alpha$ into rows so that each column represents one outgoing orthonormal vector.
  \item Eigen-decomposition of the Hermitian Gram matrix avoids direct SVD and provides numerical stability when inverting $A^\dagger A$.
  \item Clamping tiny eigenvalues to zero prevents large numerical spikes when the spectrum is ill-conditioned.
  \item After the inverse square-root is applied, the tensor is rearranged back to its original layout, yielding an isometric tensor ready for the next iteration.
\end{itemize}

\section{Practical Implications}
\begin{itemize}
  \item Presenting the optimisation in terms of $U$ only highlights the part of the network that is tuned in practice and avoids introducing tensors that stay fixed.
  \item Additional MERA components (such as disentanglers) can be incorporated in future work by extending Algorithm~\ref{alg:mera_opt} and reusing Algorithm~\ref{alg:tensor_update_svd} with the appropriate cut axis.
  \item The combination of sparsity and optional MSE loss provides a trade-off between compression and fidelity; tuning $\lambda_{\text{MSE}}$ activates the reconstruction branch explicitly.
\end{itemize}

\appendix
\section{Numeric Example for $\chi = 2$}
\label{sec:chi-two-example}
Consider a single MERA level where the isometry $U^{(1)}$ joins two child subbands into one parent channel.  To reproduce the behaviour observed in the code, proceed as follows:
\begin{enumerate}
  \item Take one window from the telemetry trace and run the forward MERA transform until level $L=1$.  You obtain two complex child detail vectors, each of length $2$ after down-sampling.
  \item Stack the child coefficients into a length-4 vector $\mathbf{f} = (f_{1\uparrow}, f_{1\downarrow}, f_{2\uparrow}, f_{2\downarrow})^\top$ by interleaving the approximation ($\uparrow$) and detail ($\downarrow$) channels from the two legs, matching the memory layout used by \texttt{Wave6G.mera\_analyze}.
  \item Use a $\chi = 2$ warm start (or extract the learned tensors from \texttt{train\_\*.csv}) and reshape $U^{(1)}$ into the $2 \times 4$ matrix
  \[
  U =
  \begin{bmatrix}
  0.7 & 0.0 & 0.7 & 0.0 \\
  0.0 & 0.7 & 0.0 & -0.7
  \end{bmatrix}.
  \]
  The two rows correspond to the two parent channels that survive after this MERA layer.
  \item Multiply $\mathbf{f}$ by $U$.  For example:
  \begin{itemize}
    \item If $\mathbf{f} = (1, 0, 0, 0)^\top$ (only the first coefficient active), the parent output is $(0.7, 0)^\top$; all energy flows through the first parent channel.
    \item If $\mathbf{f} = (0, 1, 0, 1)^\top$, the result is $(0, 0)^\top$, meaning this specific combination is suppressed entirely.  You can verify this quickly in Julia via `U * [0,1,0,1]`.
  \end{itemize}
\end{enumerate}
Because $U$ has only two rows, \emph{any} child vector ultimately lies in a two-dimensional subspace.  Thus at most two independent mixtures of child subbands can reach the next scale, which mirrors the compression effect observed in the pipeline when $\chi=2$.

\section{Numeric Example for $\chi = 4$}
Repeat the previous steps but initialise (or extract) tensors with $\chi = 4$.  After reshaping, one valid isometry is
\[
U_{(4)} =
\begin{bmatrix}
0.5 & 0.0 & 0.5 & 0.0 \\
0.0 & 0.5 & 0.0 & 0.5 \\
0.5 & 0.0 & -0.5 & 0.0 \\
0.0 & 0.5 & 0.0 & -0.5
\end{bmatrix}.
\]
To reproduce the effect:
\begin{enumerate}
  \item Use the same child vector $\mathbf{f}$ produced earlier (the interleaved approximation/detail layout is unchanged).
  \item Multiply by $U_{(4)}$.  Two illustrative cases:
  \begin{itemize}
    \item $\mathbf{f} = (1, 0, 0, 0)^\top$ yields $(0.5, 0.0, 0.5, 0.0)^\top$, i.e., the energy splits evenly between two parent channels.
    \item $\mathbf{f} = (0, 1, 0, -1)^\top$ gives $(0.0, 1.0, 0.0, -1.0)^\top$, preserving the contrast between the two legs—something impossible with $\chi=2$.
  \end{itemize}
\end{enumerate}
Having four rows raises the rank ceiling to $4$, so up to four independent mixtures propagate upward.  In practice this translates to richer filters but also higher compute and memory cost, reproducing the trade-off seen when running the Julia pipeline with larger $\chi$.

\section{Worked 8-Sample Signal}
To make the bond-dimension effect completely concrete, take the window
\[
x = (1.2,\; 0.1,\; -0.6,\; 0.4,\; 0.9,\; -0.2,\; 0.3,\; -0.5).
\]
\begin{enumerate}
  \item \textbf{Level-1 analysis (Haar).}  Level~1 is the first decomposition right after the raw signal.  Using the Haar filters adopted by the pipeline,
  \[
  \text{approx}_k = \frac{x_{2k-1} + x_{2k}}{\sqrt{2}}, \qquad
  \text{detail}_k = \frac{x_{2k-1} - x_{2k}}{\sqrt{2}}.
  \]
  Evaluating these expressions yields
  \[
  \begin{aligned}
  &\text{approx} = (0.9192,\; -0.1414,\; 0.4950,\; -0.1414),\\
  &\text{detail} = (0.7778,\; -0.7071,\; 0.7778,\; 0.5657),
  \end{aligned}
  \]
  where each entry corresponds to one child leg (two original samples).
  \item \textbf{Pass approximations upward.}  Mimicking the diagram, the detail coefficients $d^{(1)}_k$ leave the tree immediately, while the approximations continue upward.  For the block covering $x_1,\ldots,x_4$, stack
  \[
  \mathbf{f}_a = (a^{(1)}_1,\; a^{(1)}_2)^\top = (0.9192,\; -0.1414)^\top.
  \]
  \item \textbf{Propagate with $\chi = 2$.}  Choose any $2\times 2$ isometry (one row yields $a^{(2)}$, the other $d^{(2)}$), e.g.
  \[
  U_{(2)} \mathbf{f}_a = 
  \begin{bmatrix}
  0.8 & 0.6 \\
  -0.6 & 0.8
  \end{bmatrix}
  \mathbf{f}_a
  =
  (0.6500,\; -0.8716)^\top.
  \]
  This multiplication is a standard matrix--vector product: each row takes a weighted sum of the stacked approximations and produces one coarse-scale channel.  Because $U_{(2)}$ has exactly two rows, the output vector has length~2, so only two parent channels (the approximation/detail pair at the next level) are produced regardless of how many child legs exist.
  \item \textbf{Propagate with $\chi = 4$.}  If the upper isometry had $\chi = 4$, we would instead use a \(4 \times 2\) matrix whose rows form four orthonormal combinations of $(a^{(1)}_1, a^{(1)}_2)$.  Four entries would then survive to the next level, representing four distinct coarse-scale approximation/detail channels.
\end{enumerate}
Repeating the same procedure for the remaining child legs (derived from samples 5--8) completes the level-1 sweep.  Subsequent MERA levels repeat the process: the newly created parent coefficients become the “children” for level~2, and the chosen $\chi$ dictates how many mixtures of approximation/detail information continue to propagate.

\section{Worked 4-Sample, Two-Level Pipeline}
To see every isometry application end-to-end, take a minimal window of four samples
\[
x = (1.0,\; 0.2,\; -0.4,\; 0.8)
\]
and choose $L = 2$, $\chi = 2$.  The pipeline then executes:
\begin{enumerate}
  \item \textbf{Level-1 Haar (two $U_1$ blocks).}  Applying the Haar filters to $x_1,x_2$ and $x_3,x_4$ gives
  \[
  a^{(1)}_1 = 0.8485,\quad d^{(1)}_1 = 0.5657,\qquad
  a^{(1)}_2 = 0.2828,\quad d^{(1)}_2 = -0.8485.
  \]
  The detail coefficients $d^{(1)}_1, d^{(1)}_2$ already correspond to the outgoing wires in the diagram; they do not interact further with upper levels.
  \item \textbf{Stack approximations for level~2.}  Only approximations flow upward, so build $\mathbf{f}^{(2)} = (a^{(1)}_1,\; a^{(1)}_2)^\top = (0.8485,\; 0.2828)^\top$.
  \item \textbf{Apply top isometry $U_2$.}  Using the same $2\times 2$ orthonormal matrix as above,
  \[
  U_2 \mathbf{f}^{(2)} =
  \begin{bmatrix}
    0.8 & 0.6 \\
    -0.6 & 0.8
  \end{bmatrix}
  \mathbf{f}^{(2)} =
  (1.2275,\; 0.3168)^\top,
  \]
  which yields the coarse-scale approximation/detail pair $a^{(2)}_1, d^{(2)}_1$.
\end{enumerate}
Because $\chi = 2$ across both levels, every stage emits exactly two channels, so only two independent mixtures propagate upward at each step.  Increasing $\chi$ to $4$ would lengthen both $\mathbf{f}^{(1)}$ and $\mathbf{f}^{(2)}$ outputs, resulting in four coarse-scale channels and, consequently, richer reconstructions.

\subsection*{Variation: $\chi = 4$ at the Top}
If we artificially enforce $\chi = 4$ for $U_2$ while still feeding only the two approximations $(a^{(1)}_1, a^{(1)}_2)$, the isometry must be \(4 \times 2\):
\[
U_2^{(\chi=4)} =
\begin{bmatrix}
  0.5 & 0.5 \\
  0.5 & -0.5 \\
  0.5 & 0.5 \\
  -0.5 & 0.5
\end{bmatrix},
\quad
U_2^{(\chi=4)} \mathbf{f}^{(2)} = (0.5656,\; 0.2828,\; 0.5656,\; -0.2828)^\top.
\]
The four outputs are just replicated/scaled versions of the two inputs, so no additional information is created: the rank remains at most 2.  This illustrates why using a higher $\chi$ without supplying more independent child approximations only adds redundancy.

To link this example to the schematic shown in the prompt (two $U_1$ tensors feeding a top $U_2$), note the following identifications for the numeric values above:
\begin{align*}
  a^{(1)}_1 &= 0.8485, & d^{(1)}_1 &= 0.5657 \quad &&\text{(left $U_1$ fed by $x_1,x_2$)} \\
  a^{(1)}_2 &= 0.2828, & d^{(1)}_2 &= -0.8485 \quad &&\text{(right $U_1$ fed by $x_3,x_4$)} \\
  a^{(2)}_1 &= 1.2275, & d^{(2)}_1 &= 0.3168 \quad &&\text{(outputs of the top $U_2$).}
\end{align*}
Thus each wire in the diagram corresponds directly to one of the entries in the worked example, making it straightforward to trace how the raw samples are routed through $U_1$ and then $U_2$.

\end{document}
